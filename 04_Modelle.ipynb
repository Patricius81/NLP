{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import zipfile\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import TfidfModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA (Latent Dirichlet Allocation), NMF (Non-Negative Matrix Factorization), and BERTopic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>preprocessed_reviewText</th>\n",
       "      <th>tokens</th>\n",
       "      <th>review_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>material arrive early excellent condition howe...</td>\n",
       "      <td>[material, arrive, early, excellent, condition...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>really enjoy book worksheet make review goal m...</td>\n",
       "      <td>[really, enjoy, book, worksheet, make, review,...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "      <td>take class do nt waste money call book book is...</td>\n",
       "      <td>[take, class, do, nt, waste, money, call, book...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucy</td>\n",
       "      <td>This book was missing pages!!! Important pages...</td>\n",
       "      <td>missing pages!!</td>\n",
       "      <td>book miss page important page could answer tes...</td>\n",
       "      <td>[book, miss, page, important, page, could, ans...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albert V.</td>\n",
       "      <td>I have used LearnSmart and can officially say ...</td>\n",
       "      <td>Best study product out there!</td>\n",
       "      <td>use learnsmart officially say amazing study to...</td>\n",
       "      <td>[use, learnsmart, officially, say, amazing, st...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reviewerName                                         reviewText  \\\n",
       "0           Michelle W  The materials arrived early and were in excell...   \n",
       "1  Rosalind White Ames  I am really enjoying this book with the worksh...   \n",
       "2       Allan R. Baker  IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...   \n",
       "3                 Lucy  This book was missing pages!!! Important pages...   \n",
       "4            Albert V.  I have used LearnSmart and can officially say ...   \n",
       "\n",
       "                         summary  \\\n",
       "0                 Material Great   \n",
       "1                         Health   \n",
       "2             ARE YOU KIDING ME?   \n",
       "3                missing pages!!   \n",
       "4  Best study product out there!   \n",
       "\n",
       "                             preprocessed_reviewText  \\\n",
       "0  material arrive early excellent condition howe...   \n",
       "1  really enjoy book worksheet make review goal m...   \n",
       "2  take class do nt waste money call book book is...   \n",
       "3  book miss page important page could answer tes...   \n",
       "4  use learnsmart officially say amazing study to...   \n",
       "\n",
       "                                              tokens  review_lengths  \n",
       "0  [material, arrive, early, excellent, condition...              13  \n",
       "1  [really, enjoy, book, worksheet, make, review,...              12  \n",
       "2  [take, class, do, nt, waste, money, call, book...              28  \n",
       "3  [book, miss, page, important, page, could, ans...              11  \n",
       "4  [use, learnsmart, officially, say, amazing, st...              50  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'preprocessed_reviews.pkl'\n",
    "\n",
    "# Read the pickled file directly into a DataFrame\n",
    "reviews = pd.read_pickle(file)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random selection of 10000 reviews\n",
    "reviews_10000 = reviews.sample(n=10000, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing TF IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of lists containing words for each review text (preprocessed)\n",
    "#docs = reviews['preprocessed_reviewText'].tolist()\n",
    "docs = reviews_10000['preprocessed_reviewText'].tolist()\n",
    "\n",
    "# convert to a list of lists\n",
    "docs = [doc.split() for doc in docs]\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF Transformation\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the TF-IDF transformed corpus and texts into training and testing sets  only for lda_model_8020.sav\n",
    "train_corpus, test_corpus, train_texts, test_texts = train_test_split(tfidf_corpus, docs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA Model using TF-IDF on the training set\n",
    "lda_model2 = LdaModel(corpus=tfidf_corpus, id2word=dictionary, num_topics=5, passes=20)\n",
    "\n",
    "# 71 min for lda_model 80/20 split, 5 topics, 50 passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to disk\n",
    "pickle.dump(lda_model2, open('lda_model_10kR.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model from disk\n",
    "lda_model2 = pickle.load(open('lda_model_10kR.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_model_10kR.sav\n",
      "(0, '0.017*\"tax\" + 0.010*\"easy\" + 0.010*\"taxis\" + 0.009*\"year\" + 0.009*\"turbo\"')\n",
      "(1, '0.029*\"great\" + 0.028*\"good\" + 0.016*\"work\" + 0.011*\"ok\" + 0.008*\"advertise\"')\n",
      "(2, '0.011*\"love\" + 0.002*\"bible\" + 0.001*\"nancy\" + 0.001*\"_\" + 0.001*\"niece\"')\n",
      "(3, '0.004*\"payroll\" + 0.003*\"expectation\" + 0.003*\"meet\" + 0.002*\"chrome\" + 0.002*\"cell\"')\n",
      "(4, '0.005*\"use\" + 0.005*\"work\" + 0.005*\"product\" + 0.005*\"program\" + 0.005*\"software\"')\n"
     ]
    }
   ],
   "source": [
    "# Print topics\n",
    "print(\"lda_model_10kR.sav\")\n",
    "\n",
    "topics = lda_model2.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model from disk\n",
    "lda_model = pickle.load(open('lda_model_8020.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_model_8020.sav\n",
      "(0, '0.005*\"window\" + 0.005*\"use\" + 0.005*\"program\" + 0.004*\"work\" + 0.004*\"software\"')\n",
      "(1, '0.037*\"game\" + 0.013*\"play\" + 0.012*\"kid\" + 0.012*\"fun\" + 0.011*\"map\"')\n",
      "(2, '0.029*\"ok\" + 0.025*\"advertise\" + 0.021*\"awesome\" + 0.020*\"describe\" + 0.016*\"thank\"')\n",
      "(3, '0.010*\"year\" + 0.008*\"use\" + 0.007*\"tax\" + 0.006*\"quicken\" + 0.006*\"product\"')\n",
      "(4, '0.083*\"great\" + 0.054*\"good\" + 0.048*\"work\" + 0.031*\"love\" + 0.027*\"product\"')\n"
     ]
    }
   ],
   "source": [
    "# Print topics\n",
    "print(\"lda_model_8020.sav\")\n",
    "\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -9.039971818437143\n",
      "\n",
      "Coherence Score2:  0.5169026087556776\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Lower perplexity and higher coherence scores indicate better model performance.\n",
    "\n",
    "# Calculate Perplexity\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(test_corpus))\n",
    "print('\\nPerplexity: ', lda_model2.log_perplexity(tfidf_corpus))\n",
    "\n",
    "# Calculate Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model=lda_model, texts=test_texts, dictionary=dictionary, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda2 = CoherenceModel(model=lda_model2, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda2 = coherence_model_lda2.get_coherence()\n",
    "print('\\nCoherence Score2: ', coherence_lda2)\n",
    "\n",
    "\n",
    "# lda_model_80/20  Perplexity:  -9.585373895318991 , Coherence Score:  0.35134297496128297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity: The log_perplexity method computes the model’s perplexity on the test set. It’s a measure of how well the model predicts the holdout sample and \n",
    "# is often used for comparing different topic models. Lower perplexity values indicate better generalization performance.\n",
    "\n",
    "# Coherence Score: The CoherenceModel class from Gensim is used to compute the coherence score of the LDA model. A higher coherence score indicates that the words\n",
    "# comprising the topics are more semantically related, making the topics more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dominant topic to each review\n",
    "dominant_topics = []\n",
    "for doc_bow in corpus:\n",
    "    topic_probs = lda_model.get_document_topics(doc_bow)\n",
    "    dominant_topic = sorted(topic_probs, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    dominant_topics.append(dominant_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_model_8020.sav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic  count\n",
       "0      0   9305\n",
       "1      1     12\n",
       "2      2     77\n",
       "3      3    594\n",
       "4      4     12"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"lda_model_8020.sav\")\n",
    "\n",
    "# count how many diferent topics there are:\n",
    "unique, counts = np.unique(dominant_topics, return_counts=True)\n",
    "\n",
    "# create table with topics and counts\n",
    "topics_counts = pd.DataFrame({'topic':unique, 'count':counts})\n",
    "topics_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dominant topic to each review\n",
    "dominant_topics2 = []\n",
    "for doc_bow in corpus:\n",
    "    topic_probs = lda_model2.get_document_topics(doc_bow)\n",
    "    dominant_topic2 = sorted(topic_probs, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    dominant_topics2.append(dominant_topic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_model_10kR.sav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic  count\n",
       "0      0    634\n",
       "1      1    412\n",
       "2      2     67\n",
       "3      3     20\n",
       "4      4   8867"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"lda_model_10kR.sav\")\n",
    "\n",
    "# count how many diferent topics there are:\n",
    "unique, counts = np.unique(dominant_topics2, return_counts=True)\n",
    "\n",
    "# create table with topics and counts\n",
    "topics_counts2 = pd.DataFrame({'topic':unique, 'count':counts})\n",
    "topics_counts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dominant topic to dataframe\n",
    "reviews_10000['Topic_LDA_model_8020'] = dominant_topics\n",
    "reviews_10000['Topic_LDA_model_10kR'] = dominant_topics2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2 = reviews_10000['preprocessed_reviewText'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X = vectorizer.fit_transform(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5  \n",
    "\n",
    "nmf = NMF(n_components=num_topics, random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: software, get, program, version, computer\n",
      "Topic 1: great, product, price, item, deal\n",
      "Topic 2: good, product, price, far, software\n",
      "Topic 3: work, well, window, fine, expect\n",
      "Topic 4: use, easy, year, tax, love\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_features = [feature_names[i] for i in topic.argsort()[-5:][::-1]]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asign topic to each review\n",
    "reviews_10000['Topic_NMF'] = W.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>preprocessed_reviewText</th>\n",
       "      <th>tokens</th>\n",
       "      <th>review_lengths</th>\n",
       "      <th>Topic_NMF</th>\n",
       "      <th>Topic_LDA_model_8020</th>\n",
       "      <th>Topic_LDA_model_10kR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>417709</th>\n",
       "      <td>steve hayes</td>\n",
       "      <td>Simply does not work. I have a Panasonic camco...</td>\n",
       "      <td>Don't do it</td>\n",
       "      <td>simply work panasonic camcorder think would co...</td>\n",
       "      <td>[simply, work, panasonic, camcorder, think, wo...</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244812</th>\n",
       "      <td>Lori Evans</td>\n",
       "      <td>I ordered the cyberlink photo software &amp; recei...</td>\n",
       "      <td>I ordered the cyberlink photo software &amp; recei...</td>\n",
       "      <td>order cyberlink photo software receive can not...</td>\n",
       "      <td>[order, cyberlink, photo, software, receive, c...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144600</th>\n",
       "      <td>Brncs</td>\n",
       "      <td>Had to upgrade for bank downloads to work. Wou...</td>\n",
       "      <td>Not bad</td>\n",
       "      <td>upgrade bank download work would prefer job re...</td>\n",
       "      <td>[upgrade, bank, download, work, would, prefer,...</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285232</th>\n",
       "      <td>Geoman</td>\n",
       "      <td>Do not buy this.  I have been buying Turbo tax...</td>\n",
       "      <td>bait-and-switch</td>\n",
       "      <td>buy buying turbo tax last year never unlike ye...</td>\n",
       "      <td>[buy, buying, turbo, tax, last, year, never, u...</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443181</th>\n",
       "      <td>Willie Broughton</td>\n",
       "      <td>Users were very please with the Word perfect S...</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>user please word perfect suite</td>\n",
       "      <td>[user, please, word, perfect, suite]</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248874</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>SIDS not work as instructed</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>sid work instruct</td>\n",
       "      <td>[sid, work, instruct]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115195</th>\n",
       "      <td>I love it</td>\n",
       "      <td>&lt;a data-hook=\"product-link-linked\" class=\"a-li...</td>\n",
       "      <td>Kaspersky Anti-Virus</td>\n",
       "      <td>datahookproductlinklinked classalinknormal hre...</td>\n",
       "      <td>[datahookproductlinklinked, classalinknormal, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281814</th>\n",
       "      <td>Shirley Dorris</td>\n",
       "      <td>Complete, fast, easy to use</td>\n",
       "      <td>easy to</td>\n",
       "      <td>complete fast easy use</td>\n",
       "      <td>[complete, fast, easy, use]</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301120</th>\n",
       "      <td>Dennis T Flick</td>\n",
       "      <td>Did the job well</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>job well</td>\n",
       "      <td>[job, well]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395121</th>\n",
       "      <td>Krista</td>\n",
       "      <td>None of the games can be used by just tapping ...</td>\n",
       "      <td>Cute Games for kids who can use a mouse</td>\n",
       "      <td>none game use tap key require use mouse child ...</td>\n",
       "      <td>[none, game, use, tap, key, require, use, mous...</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerName                                         reviewText  \\\n",
       "417709       steve hayes  Simply does not work. I have a Panasonic camco...   \n",
       "244812        Lori Evans  I ordered the cyberlink photo software & recei...   \n",
       "144600             Brncs  Had to upgrade for bank downloads to work. Wou...   \n",
       "285232            Geoman  Do not buy this.  I have been buying Turbo tax...   \n",
       "443181  Willie Broughton  Users were very please with the Word perfect S...   \n",
       "248874   Amazon Customer                        SIDS not work as instructed   \n",
       "115195         I love it  <a data-hook=\"product-link-linked\" class=\"a-li...   \n",
       "281814    Shirley Dorris                        Complete, fast, easy to use   \n",
       "301120    Dennis T Flick                                   Did the job well   \n",
       "395121            Krista  None of the games can be used by just tapping ...   \n",
       "\n",
       "                                                  summary  \\\n",
       "417709                                        Don't do it   \n",
       "244812  I ordered the cyberlink photo software & recei...   \n",
       "144600                                            Not bad   \n",
       "285232                                    bait-and-switch   \n",
       "443181                                         Four Stars   \n",
       "248874                                          Two Stars   \n",
       "115195                               Kaspersky Anti-Virus   \n",
       "281814                                            easy to   \n",
       "301120                                         Five Stars   \n",
       "395121            Cute Games for kids who can use a mouse   \n",
       "\n",
       "                                  preprocessed_reviewText  \\\n",
       "417709  simply work panasonic camcorder think would co...   \n",
       "244812  order cyberlink photo software receive can not...   \n",
       "144600  upgrade bank download work would prefer job re...   \n",
       "285232  buy buying turbo tax last year never unlike ye...   \n",
       "443181                     user please word perfect suite   \n",
       "248874                                  sid work instruct   \n",
       "115195  datahookproductlinklinked classalinknormal hre...   \n",
       "281814                             complete fast easy use   \n",
       "301120                                           job well   \n",
       "395121  none game use tap key require use mouse child ...   \n",
       "\n",
       "                                                   tokens  review_lengths  \\\n",
       "417709  [simply, work, panasonic, camcorder, think, wo...              31   \n",
       "244812  [order, cyberlink, photo, software, receive, c...              13   \n",
       "144600  [upgrade, bank, download, work, would, prefer,...              11   \n",
       "285232  [buy, buying, turbo, tax, last, year, never, u...              19   \n",
       "443181               [user, please, word, perfect, suite]               5   \n",
       "248874                              [sid, work, instruct]               3   \n",
       "115195  [datahookproductlinklinked, classalinknormal, ...              10   \n",
       "281814                        [complete, fast, easy, use]               4   \n",
       "301120                                        [job, well]               2   \n",
       "395121  [none, game, use, tap, key, require, use, mous...              21   \n",
       "\n",
       "        Topic_NMF  Topic_LDA_model_8020  Topic_LDA_model_10kR  \n",
       "417709          0                     0                     4  \n",
       "244812          0                     0                     4  \n",
       "144600          3                     3                     4  \n",
       "285232          4                     0                     4  \n",
       "443181          0                     0                     4  \n",
       "248874          3                     3                     1  \n",
       "115195          1                     0                     4  \n",
       "281814          4                     0                     0  \n",
       "301120          3                     3                     1  \n",
       "395121          4                     0                     4  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_10000.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Users were very please with the Word perfect Suite'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_10000[\"reviewText\"].iloc[4]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
